{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as thub\n",
    "import bert\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(context):\n",
    "    \n",
    "    \"\"\"\n",
    "    To be applied over Spark dataframe.\n",
    "    Takes a string and converts it to token IDs via bert_tokenizer,\n",
    "    adding the necessary beginning and end tokens\n",
    "\n",
    "    Returns: Array of bert token ids for each row of Spark dataframe (requires udf)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized = [\"[CLS]\"] + tokenizer.tokenize(context) + [\"[SEP]\"]\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epoch_df(sarcastic, non_sarcastic, ratio, \n",
    "                      n_epochs, bucket_by_lengths=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns: Spark df of equal label distribution with text \n",
    "    tokenized. Each generated df is to be iterator over multiple \n",
    "    times during training\n",
    "    \"\"\"\n",
    "    \n",
    "    number = 0\n",
    "    while number < n_epochs:\n",
    "        non_sarc_samp = non_sarcastic.sample(ratio) # making label dist equal\n",
    "        \n",
    "        # combine sampled non_sarcastic and whole sarcastic\n",
    "        epoch_df = sarcastic.union(non_sarc_samp)\n",
    "        \n",
    "        # tokenize context column via spark udf\n",
    "        tokenize_sample_udf = F.udf(tokenize_sample, ArrayType(IntegerType()))\n",
    "        epoch_df = epoch_df.withColumn(\"tokens\", tokenize_sample_udf(epoch_df.context))\n",
    "        \n",
    "        # drop context column\n",
    "        epoch_df = epoch_df.drop(\"context\")\n",
    "        \n",
    "        # yield one call at a time\n",
    "        yield epoch_df\n",
    "        number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_comment_length(epoch_df, batch_size=16):\n",
    "    \n",
    "    # add sequence lengths\n",
    "    epoch_df = epoch_df.withColumn(\"sequence_length\", F.size(epoch_df.tokens))\n",
    "        \n",
    "    # order by sequence length\n",
    "    epoch_df = epoch_df.orderBy(\"sequence_length\", ascending=False)\n",
    "            \n",
    "    # drop sequence length column\n",
    "    epoch_df = epoch_df.drop(\"sequence_length\")\n",
    "            \n",
    "    # convert pandas\n",
    "    epoch_df = epoch_df.toPandas()\n",
    "    \n",
    "    # convert to sorted list of tuples\n",
    "    sorted_tokens = [(epoch_df['tokens'].iloc[i], epoch_df['label'].iloc[i]) for i in range(len(epoch_df))]\n",
    "    \n",
    "    return sorted_tokens\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_by_overall_length(epoch_df):\n",
    "    \n",
    "    # Start with three empty lists and initialize var for max_seq_len\n",
    "    tokens, X, y = [], [], []\n",
    "    max_seq_len = 0\n",
    "    \n",
    "    # convert pandas\n",
    "    epoch_df = epoch_df.toPandas()\n",
    "    \n",
    "    # generator for iterating over df\n",
    "    for _, row in tqdm(epoch_df.iterrows()):\n",
    "        \n",
    "        # pull out raw tokens and label for each row\n",
    "        raw_tokens, label = row[epoch_df.tokens], row[epoch_df.label]\n",
    "        \n",
    "        # update max sequence length var\n",
    "        max_seq_len = max(max_seq_len, len(raw_tokens))\n",
    "      \n",
    "        # append results as list to empty list\n",
    "        tokens.append(raw_tokens)\n",
    "        y.append(label)\n",
    "    \n",
    "    # convert response to nparray\n",
    "    y = np.array(y)\n",
    "\n",
    "    # for each of the raw tokens\n",
    "    for sample in tokens:\n",
    "        \n",
    "        # truncate sample list if for some reason max_seq_len is shorter than actual length \n",
    "        sample = sample[:min(len(sample), max_seq_len - 2)]\n",
    "        \n",
    "        # add zeros to pad the elements if length of sample is less then max_seq_len\n",
    "        sample = sample + [0] * (max_seq_len - len(sample))\n",
    "        \n",
    "        # append result to empty list X\n",
    "        X.append(np.array(sample))\n",
    "    \n",
    "    # convert predictor to nparray\n",
    "    X = np.array(X)\n",
    "    \n",
    "    return X,y\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_seq_len, bert_ckpt_file):\n",
    "    \n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "\n",
    "    input_tokens = keras.layers.Input(shape=(max_seq_len, ),\n",
    "                                   dtype='int64',\n",
    "                                   name=\"input_tokens\")\n",
    "  bert_output = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", bert_output.shape)\n",
    "\n",
    "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "  logits = keras.layers.Dropout(0.5)(logits)\n",
    "  logits = keras.layers.Dense(\n",
    "    units=len(classes),\n",
    "    activation=\"softmax\"\n",
    "  )(logits)\n",
    "\n",
    "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 3.92 s, total: 27.5 s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT model and tokenizer\n",
    "\n",
    "%time bert_layer, tokenizer = model_utils.init_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.3 ms, sys: 14.3 ms, total: 51.6 ms\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark context\n",
    "\n",
    "%time sc, spark = model_utils.init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 7.91 ms, total: 19.3 ms\n",
      "Wall time: 42.2 s\n"
     ]
    }
   ],
   "source": [
    "# Read in sarcastic samples, non-sarcastic samples, and the ratio between the two\n",
    "\n",
    "%time sarcastic, non_sarcastic, ratio = model_utils.load_data(spark, bucket_name=\"sarc-bucket-5\", dataset=\"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize epoch_df generator\n",
    "\n",
    "%time generator = generate_epoch_df(sarcastic, non_sarcastic, ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 277 ms, sys: 3.41 ms, total: 281 ms\n",
      "Wall time: 384 ms\n"
     ]
    }
   ],
   "source": [
    "%time epoch_df = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = sort_by_comment_length(epoch_df, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dataset = tf.data.Dataset.from_generator(lambda: sorted_tokens, output_types=(tf.int64, tf.int64))\n",
    "padded_batch = sorted_dataset.padded_batch(16, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16, 1311), dtype=int64, numpy=\n",
       " array([[  101,   107,  3446, ...,  1122,   119,   102],\n",
       "        [  101,   107,  1212, ...,     0,     0,     0],\n",
       "        [  101,   107,  2431, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   107, 15859, ...,     0,     0,     0],\n",
       "        [  101,   107,  1135, ...,     0,     0,     0],\n",
       "        [  101,  1789,  1104, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(16,), dtype=int64, numpy=array([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1])>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(padded_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|              tokens|\n",
      "+-----+--------------------+\n",
      "|    1|[101, 4208, 117, ...|\n",
      "|    1|[101, 12357, 112,...|\n",
      "|    1|[101, 5651, 5797,...|\n",
      "|    1|[101, 107, 107, 1...|\n",
      "|    1|[101, 10364, 119,...|\n",
      "|    1|[101, 107, 107, 1...|\n",
      "|    1|[101, 18725, 117,...|\n",
      "|    1|[101, 2814, 117, ...|\n",
      "|    1|[101, 3046, 1190,...|\n",
      "|    1|[101, 146, 2810, ...|\n",
      "|    1|[101, 11205, 119,...|\n",
      "|    1|[101, 1753, 1155,...|\n",
      "|    1|[101, 119, 119, 1...|\n",
      "|    1|[101, 1124, 1108,...|\n",
      "|    1|[101, 5704, 1103,...|\n",
      "|    1|[101, 27020, 160,...|\n",
      "|    1|[101, 107, 14180,...|\n",
      "|    1|[101, 1103, 1234,...|\n",
      "|    1|[101, 17135, 2707...|\n",
      "|    1|[101, 1192, 1274,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
