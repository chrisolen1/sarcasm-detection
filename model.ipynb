{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as thub\n",
    "import bert\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import model_utils\n",
    "\n",
    "# maximum length of token sequences to input to bert model\n",
    "max_seq_length = 128\n",
    "\n",
    "# number of smaples to generate from the distributed database\n",
    "n_samples = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify NVIDIA drivers are install successfully\n",
    "\n",
    "!sudo nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(context):\n",
    "    \n",
    "    \"\"\"\n",
    "    To be applied over Spark dataframe.\n",
    "    Takes a string and converts it to token IDs via bert_tokenizer,\n",
    "    adding the necessary beginning and end tokens\n",
    "\n",
    "    Returns: Array of bert token ids for each row of Spark dataframe (requires udf)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized = [\"[CLS]\"] + tokenizer.tokenize(context) + [\"[SEP]\"]\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_df(sarcastic, non_sarcastic, ratio, n_samples, max_seq_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns: Spark df of equal label distribution with text \n",
    "    tokenized. Each generated df is to be iterator over multiple \n",
    "    times during training\n",
    "    \"\"\"\n",
    "    \n",
    "    number = 0\n",
    "    while number < n_samples:\n",
    "        \n",
    "        non_sarc_samp = non_sarcastic.sample(ratio) # making label dist equal\n",
    "        \n",
    "        # combine sampled non_sarcastic and whole sarcastic\n",
    "        sample_df = sarcastic.union(non_sarc_samp)\n",
    "        \n",
    "        # tokenize context column via spark udf\n",
    "        tokenize_sample_udf = F.udf(tokenize_sample, ArrayType(IntegerType()))\n",
    "        sample_df = sample_df.withColumn(\"raw_tokens\", tokenize_sample_udf(sample_df.context))\n",
    "        # keep only the first 'max_seq_length' tokens\n",
    "        sample_df = sample_df.withColumn(\"tokens\", F.slice('raw_tokens',1, max_seq_length))\n",
    "        \n",
    "        # drop context and raw_tokens columns\n",
    "        sample_df = sample_df.drop(\"context\")\n",
    "        sample_df = sample_df.drop(\"raw_tokens\")\n",
    "        \n",
    "        # yield one call at a time\n",
    "        yield sample_df\n",
    "        number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "        n_fine_tune_layers=2,\n",
    "        output_type=\"sequence_output\",\n",
    "        bert_path=\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs):\n",
    "        \n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.output_type = output_type\n",
    "        self.bert_path = bert_path\n",
    "        \n",
    "        if self.output_type not in [\"sequence_output\", \"pooled_output\"]:\n",
    "            raise NameError(\"Undefined pooling type (must be either sequence_output or pooled_output, but is {self.output_type}\")\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def layer_number(self, var):\n",
    "    \n",
    "        \"\"\"\n",
    "        Get the layer number corresponding to the \n",
    "        given variable\n",
    "        \"\"\"\n",
    "        m = re.search(r'/layer_(\\d+)/', var)\n",
    "        \n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        \"\"\"\n",
    "        Creates the variables of the layer (optional, for subclass implementers).\n",
    " |      \n",
    " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
    " |      can override if they need a state-creation step in-between\n",
    " |      layer instantiation and layer call.\n",
    " |      \n",
    " |      This is typically used to create the weights of `Layer` subclasses.\n",
    " \n",
    "        Called once from `__call__`, when we know the shapes of input and `dtype`.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.bert_model = thub.KerasLayer(self.bert_path, self.trainable)\n",
    "\n",
    "        # extract all trainable variables from the model\n",
    "        trainable_vars = self.bert_model.trainable_variables\n",
    "        \n",
    "        if self.output_type == \"pooled_output\":\n",
    "            \n",
    "            # removing '/cls/' layers (there don't appear to be any) \n",
    "            trainable_vars = [var.name for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            \n",
    "        elif self.output_type == \"sequence_output\":\n",
    "            \n",
    "            # removing '/cls/' (there don't appear to be any) and '/pooler_transform/' layers \n",
    "            trainable_vars = [var.name for var in trainable_vars if not \"/cls/\" in var.name\n",
    "                              and not \"/pooler_transform\" in var.name] \n",
    "            \n",
    "        ### select how many layers to fine tune starting from top-most layer ###\n",
    "        \n",
    "        # outputs a list of either Nonetype or layer number\n",
    "        layer_numbers = list(map(self.layer_number, trainable_vars))\n",
    "        # returns the total number of layers in pre-trained model (note: layers are zero-indexed)\n",
    "        n_total_layers = max(n for n in layer_numbers if n is not None) + 1 \n",
    "        # finally, create list of just layers to be trained\n",
    "        trainable_vars = [var for n, var in zip(layer_numbers, trainable_vars) if n is not None and n >= n_total_layers - self.n_fine_tune_layers]    \n",
    "        \n",
    "        # add variables NOT to be trained to _non_trainable_weights and \n",
    "        # remove them from _trainable_weights\n",
    "        # note: underscore is necessary for accessing the writable object\n",
    "        for var in self.bert_model.variables:\n",
    "\n",
    "            if var.name not in trainable_vars and \"Variable:0\" not in var.name:\n",
    "                    \n",
    "                    # add non_trainable weights to _non_trainable_weights\n",
    "                    self.bert_model._non_trainable_weights.append(var)\n",
    "                    \n",
    "                    ### due to boolean-related issues with nparrays, need to take different approach to removing weights ###\n",
    "                    # pull out index in _trainable_weights corresponding to the var.name that needs to be removed\n",
    "                    ix = [(i,j.name) for i,j in enumerate(self.bert_model._trainable_weights) if j.name == var.name][0][0]\n",
    "                    # pop it off\n",
    "                    self.bert_model._trainable_weights.pop(ix)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs): \n",
    "        \n",
    "        \"\"\"\n",
    "        Called in `__call__` after making sure `build()` has been called\n",
    " |      once. Should actually perform the logic of applying the layer to the\n",
    " |      input tensors (which should be passed in as the first argument).\n",
    "        \"\"\"\n",
    "        # takes in list of input tensors and casts them in Keras\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        \n",
    "        #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
    "        if self.output_type == \"pooled_output\":\n",
    "            \n",
    "            output = self.bert_model(inputs)[0]\n",
    "                \n",
    "        elif self.output_type == \"sequence_output\":\n",
    "            \n",
    "            output = self.bert_model(inputs)[1]\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        return (input_shape[0], self.output_size)\n",
    "\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                max_seq_length=128,\n",
    "                n_epochs=50,\n",
    "                batch_size=13,\n",
    "                patience=5,\n",
    "                validation_split=0.1,\n",
    "                checkpoint_dir=os.getcwd() + \"/checkpoints\",\n",
    "                saved_model_dir=os.getcwd() + \"/saved_models\",\n",
    "                pad_by_batch=False):\n",
    "    \n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.validation_split = validation_split\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "        self.pad_by_batch = pad_by_batch\n",
    "\n",
    "    def build_model(self, gpu=True): \n",
    "    \n",
    "        \"\"\"\n",
    "        Defines input shapes of bert input tensors,\n",
    "        \"\"\"\n",
    "        input_token_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "        bert_inputs = [input_tokens_ids, input_mask, segment_ids]\n",
    "    \n",
    "        if gpu == True:\n",
    "            \n",
    "            mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "            with mirrored_strategy.scope():\n",
    "                \n",
    "                # tf graph\n",
    "                bert_output = BertLayer()(bert_inputs)\n",
    "                dense_out = tf.keras.layers.Dense(self.max_seq_length, activation='relu')(bert_output)\n",
    "                dense_out = tf.keras.layers.Dropout(0.5)(dense_out)\n",
    "                logits = tf.keras.layers.Dense(1, activation='sigmoid')(dense_out)\n",
    "                \n",
    "                # define, compile model\n",
    "                model = tf.keras.models.Model(inputs=bert_inputs, outputs=logits)\n",
    "                model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "                model.summary()\n",
    "        \n",
    "        elif gpu == False:\n",
    "            \n",
    "            # tf graph\n",
    "            bert_output = BertLayer()(bert_inputs)\n",
    "            dense_out = tf.keras.layers.Dense(self.max_seq_length, activation='relu')(bert_output)\n",
    "            dense_out = tf.keras.layers.Dropout(0.5)(dense_out)\n",
    "            logits = tf.keras.layers.Dense(1, activation='sigmoid')(dense_out)\n",
    "    \n",
    "            # define, compile model\n",
    "            model = tf.keras.models.Model(inputs=bert_inputs, outputs=logits)\n",
    "            model.compile(loss='binary_crossentropy', \n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "            model.summary()\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, model, train_inputs, train_labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initiates training process using self.build_model output as input.\n",
    "        If self.pad_by_batch == False, input should be the entire epoch of \n",
    "        training inputs (including masks) and labels, as numpy arrays.\n",
    "        \"\"\"\n",
    "    \n",
    "        checkpoints = tf.keras.callbacks.ModelCheckpoint(self.checkpoint_dir, verbose=1, \n",
    "                                                     save_best_only=False,\n",
    "                                                     save_weights_only=True, mode='auto', \n",
    "                                                     save_freq='epoch')\n",
    "    \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=self.patience)\n",
    "    \n",
    "        if self.pad_by_batch == False:\n",
    "            \n",
    "            train_input_ids, train_input_masks, train_segment_ids = train_inputs\n",
    "        \n",
    "            model.fit([train_input_ids, train_input_masks, train_segment_ids], \n",
    "                  train_labels,\n",
    "                  validation_split = self.validation_split,\n",
    "                  epochs = self.n_epochs,\n",
    "                  batch_size = self.batch_size,\n",
    "                  callbacks = [checkpoints, early_stopping])\n",
    "    \n",
    "            model.save(self.saved_model_dir+'/my_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "imported = tf.saved_model.load(saved_model_dir+'/my_model.h5')\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.29 s, sys: 632 ms, total: 6.92 s\n",
      "Wall time: 6.84 s\n",
      "CPU times: user 22.6 ms, sys: 25.5 ms, total: 48.1 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT model and tokenizer\n",
    "\n",
    "%time bert_layer, tokenizer = model_utils.init_bert()\n",
    "\n",
    "# Initialize Spark context\n",
    "\n",
    "%time sc, spark = model_utils.init_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.01 ms, sys: 8.31 ms, total: 14.3 ms\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "# Read in sarcastic samples, non-sarcastic samples, and the ratio between the two\n",
    "\n",
    "%time sarcastic, non_sarcastic, ratio = model_utils.load_data(spark, \\\n",
    "                                                              bucket_name=\"sarc-bucket-5\", \\\n",
    "                                                              dataset=\"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 12.9 µs\n",
      "CPU times: user 264 ms, sys: 0 ns, total: 264 ms\n",
      "Wall time: 308 ms\n"
     ]
    }
   ],
   "source": [
    "# Initialize sample_df generator\n",
    "\n",
    "%time sample_generator = generate_sample_df(sarcastic, non_sarcastic, ratio, n_samples, max_seq_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This will create a MirroredStrategy instance which will use \n",
    "all the GPUs that are visible to TensorFlow, and use NCCL as \n",
    "the cross device communication.\n",
    "\"\"\"\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|27265|\n",
      "|    0|27188|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_samples):\n",
    "\n",
    "    # Output first smaple\n",
    "\n",
    "    %time sample_df = next(sample_generator)\n",
    "\n",
    "    # Sample df distribution \n",
    "\n",
    "    sample_df.groupBy('label').count().show()\n",
    "\n",
    "    # Initialize training class object and build Bert layer\n",
    "\n",
    "    t = training()\n",
    "    %time model = t.build_model()\n",
    "\n",
    "    # Produce padded tokens, input masks, and segment ids as nparrays\n",
    "\n",
    "    %time padded_tokens, train_labels = model_utils.pad(sample_df, t.batch_size, pad_by_batch=False)\n",
    "    %time input_mask = model_utils.input_mask(padded_tokens)\n",
    "    %time segment_id = model_utils.segment_id(padded_tokens)\n",
    "    train_inputs = [padded_tokens, input_mask, segment_id]\n",
    "\n",
    "    # Execute training\n",
    "\n",
    "    t.train_model(model, train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark context\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
