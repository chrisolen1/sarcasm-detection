{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!wget -q https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\\n!unzip -o wwm_uncased_L-24_H-1024_A-16.zip\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!wget -q https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "!unzip -o wwm_uncased_L-24_H-1024_A-16.zip\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as thub\n",
    "#import bert\n",
    "#from bert import run_classifier\n",
    "#from bert import optimization\n",
    "#from bert import tokenization\n",
    "\n",
    "import keras\n",
    "from keras_radam import RAdam\n",
    "from keras import backend as K\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = thub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/1\",\n",
    "                            trainable=True)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"science\"\n",
    "\n",
    "currentDir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "bertDir = os.path.join(currentDir, \"..\", \"wwm_cased_L-24_H-1024_A-16\")\n",
    "vocabDir = os.path.join(bertDir, \"vocab.txt\")\n",
    "driver_ip = socket.gethostbyname(socket.gethostname())\n",
    "spark_conf = [('spark.kubernetes.authenticate.caCertFile', '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'), \\\n",
    "                                   ('spark.kubernetes.authenticate.oauthTokenFile','/var/run/secrets/kubernetes.io/serviceaccount/token'), \\\n",
    "                                   ('spark.kubernetes.authenticate.driver.serviceAccountName','spark-driver-sa'), \\\n",
    "                                   ('spark.kubernetes.namespace','spark'), \\\n",
    "                                   ('spark.driver.pod.name','spark-driver'), \\\n",
    "                                   ('spark.executor.instances','16'), \\\n",
    "                                   ('spark.kubernetes.container.image','gcr.io/sarcasm-3wx3ce6drvftuy/spark-v2.4.4-worker:latest'), \\\n",
    "                                   ('spark.driver.host','spark-driver.spark.svc.cluster.local'), \\\n",
    "                                   ('spark.driver.port','29413'), \\\n",
    "                                   ('spark.driver.bindAddress',driver_ip), \\\n",
    "                                   ('spark.executor.memory','6700m'), \\\n",
    "                                   ('spark.executor.cores','1'), \\\n",
    "                                   ('spark.kubernetes.driverEnv.GCS_PROJECT_ID', 'sarcasm-3wx3ce6drvftuy'), \\\n",
    "                                   ('spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS', '/mnt/secrets/sarc-bucket-sa.json'), \\\n",
    "                                   ('spark.kubernetes.driver.secrets.sarc-bucket-sa','/mnt/secrets'), \\\n",
    "                                   ('spark.kubernetes.executor.secrets.sarc-bucket-sa','/mnt/secrets'), \\\n",
    "                                   ('spark.executorEnv.GCS_PROJECT_ID','sarcasm-3wx3ce6drvftuy'), \\\n",
    "                                   ('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS','/mnt/secrets/sarc-bucket-sa.json'), \\\n",
    "                                   ('spark.hadoop.google.cloud.auth.service.account.enable','true'), \\\n",
    "                                   ('spark.hadoop.google.cloud.auth.service.account.json.keyfile','/mnt/secrets/sarc-bucket-sa.json'), \\\n",
    "                                   ('spark.hadoop.fs.gs.project.id','sarcasm-3wx3ce6drvftuy'), \\\n",
    "                                   ('spark.hadoop.fs.gs.system.bucket','sarc-bucket-3wx3ce6drvftuy')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_spark(spark_conf, driver_ip):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize  Spark context on GKE\n",
    "    \"\"\"\n",
    "    \n",
    "    conf = pyspark.SparkConf().setAll(spark_conf)\n",
    "    spark = SparkSession.builder.master(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\\\n",
    "    .appName(\"sarc\").config(conf=conf).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    return sc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns two spark dataframes and a ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    sarc = spark.read.csv(\"gs://sarc-bucket-3wx3ce6drvftuy/{}.csv\".format(dataset), \n",
    "                          inferSchema=True, header=False, sep = '\\t')\n",
    "    \n",
    "    sarcastic = sarc.where(col('label')==1)\n",
    "    non_sarcastic = sarc.where(col('label')==0)\n",
    "    sarc_cnt = sarcastic.count()\n",
    "    non_sarc_cnt = non_sarcastic.count()\n",
    "    ratio = sarc_cnt / non_sarc_cnt\n",
    "    \n",
    "    \n",
    "    return sarcastic, non_sarcastic, ratio\n",
    "\n",
    "sarcastic, non_sarcastic, ratio = load_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(context):\n",
    "    \n",
    "    \"\"\"\n",
    "    To be applied over dataframe.\n",
    "    Takes a string and converts it to token IDs via BERT tokenizer,\n",
    "    adding the necessary beginning and end tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized = [\"[CLS]\"] + tokenizer.tokenize(context) + [\"[SEP]\"]\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    return idsno`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def createTokenizer(currentDir, bertDir, vocabDir):\n",
    "    \n",
    "    token_dict = {}\n",
    "    with codecs.open(vocabDir, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "\n",
    "    tokenizer = Tokenizer(token_dict, cased=True)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = createTokenizer(currentDir, bertDir, vocabDir)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epoch_df(sarcastic, non_sarcastic, ratio, n_epochs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a dataframe of equal label distribution over which \n",
    "    we can perform mini-batch gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    number = 0\n",
    "    while number < n_epochs:\n",
    "        non_sarc_samp = non_sarcastic.sample(ratio) # making label dist equal\n",
    "        epoch_df = sarcastic.union(non_sarc_samp)\n",
    "        X = np.array(epoch_df.select('context'))\n",
    "        y = np.array(epoch_df.select('label'))\n",
    "        \n",
    "        yield X, y\n",
    "        number += 1\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "myGenerator = numberGenerator(3)\n",
    "\n",
    "print(next(myGenerator))\n",
    "print(next(myGenerator))\n",
    "print(next(myGenerator))\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for training\n",
    "\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 7\n",
    "LR = 1e-1\n",
    "\n",
    "# loading in BERT\n",
    "\n",
    "bert = load_trained_model_from_checkpoint(config_path,\n",
    "                                         checkpoint_path,\n",
    "                                         training=True,\n",
    "                                         trainable=True,\n",
    "                                         seq_len=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
