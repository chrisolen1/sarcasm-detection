{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!wget -q https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\\n!unzip -o wwm_uncased_L-24_H-1024_A-16.zip\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!wget -q https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "!unzip -o wwm_uncased_L-24_H-1024_A-16.zip\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras_radam import RAdam\n",
    "from keras import backend as K\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"science\"\n",
    "\n",
    "currentDir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "bertDir = os.path.join(currentDir, \"..\", \"wwm_cased_L-24_H-1024_A-16\")\n",
    "vocabDir = os.path.join(bertDir, \"vocab.txt\")\n",
    "driver_ip = socket.gethostbyname(socket.gethostname())\n",
    "spark_conf = [('spark.kubernetes.authenticate.caCertFile', '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'), \\\n",
    "                                   ('spark.kubernetes.authenticate.oauthTokenFile','/var/run/secrets/kubernetes.io/serviceaccount/token'), \\\n",
    "                                   ('spark.kubernetes.authenticate.driver.serviceAccountName','spark-driver-sa'), \\\n",
    "                                   ('spark.kubernetes.namespace','spark'), \\\n",
    "                                   ('spark.driver.pod.name','spark-driver'), \\\n",
    "                                   ('spark.executor.instances','16'), \\\n",
    "                                   ('spark.kubernetes.container.image','gcr.io/sarcasm-3wx3ce6drvftuy/spark-v2.4.4-worker:latest'), \\\n",
    "                                   ('spark.driver.host','spark-driver.spark.svc.cluster.local'), \\\n",
    "                                   ('spark.driver.port','29413'), \\\n",
    "                                   ('spark.driver.bindAddress',driver_ip), \\\n",
    "                                   ('spark.executor.memory','6700m'), \\\n",
    "                                   ('spark.executor.cores','1'), \\\n",
    "                                   ('spark.kubernetes.driverEnv.GCS_PROJECT_ID', 'sarcasm-3wx3ce6drvftuy'), \\\n",
    "                                   ('spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS', '/mnt/secrets/sarc-bucket-sa.json'), \\\n",
    "                                   ('spark.kubernetes.driver.secrets.sarc-bucket-sa','/mnt/secrets'), \\\n",
    "                                   ('spark.kubernetes.executor.secrets.sarc-bucket-sa','/mnt/secrets'), \\\n",
    "                                   ('spark.executorEnv.GCS_PROJECT_ID','sarcasm-3wx3ce6drvftuy'), \\\n",
    "                                   ('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS','/mnt/secrets/sarc-bucket-sa.json'), \\\n",
    "                                   ('spark.hadoop.google.cloud.auth.service.account.enable','true'), \\\n",
    "                                   ('spark.hadoop.google.cloud.auth.service.account.json.keyfile','/mnt/secrets/sarc-bucket-sa.json'), \\\n",
    "                                   ('spark.hadoop.fs.gs.project.id','sarcasm-3wx3ce6drvftuy'), \\\n",
    "                                   ('spark.hadoop.fs.gs.system.bucket','sarc-bucket-3wx3ce6drvftuy')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_spark(spark_conf, driver_ip):\n",
    "    \n",
    "    conf = pyspark.SparkConf().setAll(spark_conf)\n",
    "    spark = SparkSession.builder.master(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\\\n",
    "    .appName(\"sarc\").config(conf=conf).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    return sc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    \n",
    "    sarc = spark.read.csv(\"gs://sarc-bucket-3wx3ce6drvftuy/{}.csv\".format(dataset), \n",
    "                          inferSchema=True, header=False, sep = '\\t')\n",
    "    \n",
    "    sarcastic = sarc.where(col('label')==1)\n",
    "    non_sarcastic = sarc.where(col('label')==0)\n",
    "    \n",
    "    return sarcastic, non_sarcastic\n",
    "\n",
    "sarcastic, non_sarcastic = load_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokenizer(currentDir, bertDir, vocabDir):\n",
    "    \n",
    "    token_dict = {}\n",
    "    with codecs.open(vocabDir, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "\n",
    "    tokenizer = Tokenizer(token_dict, cased=True)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = createTokenizer(currentDir, bertDir, vocabDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epochs(sarcastic, non_sarcastic, n_epochs, seed=1):\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for training\n",
    "\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 7\n",
    "LR = 1e-1\n",
    "\n",
    "# loading in BERT\n",
    "\n",
    "bert = load_trained_model_from_checkpoint(config_path,\n",
    "                                         checkpoint_path,\n",
    "                                         training=True,\n",
    "                                         trainable=True,\n",
    "                                         seq_len=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-04b5739f9ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
