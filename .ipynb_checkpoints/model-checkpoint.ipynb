{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as thub\n",
    "import bert\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(context):\n",
    "    \n",
    "    \"\"\"\n",
    "    To be applied over Spark dataframe.\n",
    "    Takes a string and converts it to token IDs via bert_tokenizer,\n",
    "    adding the necessary beginning and end tokens\n",
    "\n",
    "    Returns: Array of bert token ids for each row of Spark dataframe (requires udf)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized = [\"[CLS]\"] + tokenizer.tokenize(context) + [\"[SEP]\"]\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epoch_df(sarcastic, non_sarcastic, ratio, n_epochs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns: Ndarray of equal label distribution over which \n",
    "    we can perform mini-batch gradient descent. Each generated df is\n",
    "    to be iterator over multiple times during training\n",
    "    \"\"\"\n",
    "    \n",
    "    number = 0\n",
    "    while number < n_epochs:\n",
    "        non_sarc_samp = non_sarcastic.sample(ratio) # making label dist equal\n",
    "        \n",
    "        # combine sampled non_sarcastic and whole sarcastic\n",
    "        epoch_df = sarcastic.union(non_sarc_samp)\n",
    "        \n",
    "        # tokenize context column via spark udf\n",
    "        tokenize_sample_udf = F.udf(tokenize_sample, ArrayType(IntegerType()))\n",
    "        epoch_df = epoch_df.withColumn(\"tokens\", tokenize_sample_udf(epoch_df.context))\n",
    "        \n",
    "        # split into X and y numpy arrays\n",
    "        X = np.array(epoch_df.select('tokens').toPandas())\n",
    "        y = np.array(epoch_df.select('label').toPandas())\n",
    "        \n",
    "        # yield one call at a time\n",
    "        yield X, y\n",
    "        number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT model and tokenizer\n",
    "\n",
    "%time bert_layer, tokenizer = model_utils.init_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark context\n",
    "\n",
    "%time sc, spark = model_utils.init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sarcastic samples, non-sarcastic samples, and the ratio between the two\n",
    "\n",
    "%time sarcastic, non_sarcastic, ratio = model_utils.load_data(spark, \n",
    "                                                        bucket_name=\"sarc-bucket-5\", \n",
    "                                                        dataset=\"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate epoch\n",
    "\n",
    "generator = generate_epoch_df(sarcastic, non_sarcastic, ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 273 ms, sys: 3.36 ms, total: 277 ms\n",
      "Wall time: 388 ms\n"
     ]
    }
   ],
   "source": [
    "%time X, y = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = tf.data.Dataset.from_generator(generator=elements_gen,\n",
    "                                     output_types=(tf.int64, tf.int64))\n",
    "\n",
    "def element_length_fn(x, y):\n",
    "    return tf.shape(x)[0]\n",
    "\n",
    "batch = epoch.\\\n",
    "apply(tf.data.experimental.\\\n",
    "bucket_by_sequence_length(element_length_func=\n",
    "                          element_length_fn,\n",
    "                          bucket_batch_sizes=[1,16], \n",
    "                          bucket_boundaries=[2]))\n",
    "\n",
    "# bucket batch size second element defines \n",
    "# how many samples per batch\n",
    "# upper length boundary refers to the minimum \n",
    "# allowable length of a seq \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function from_generator in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "from_generator(generator, output_types, output_shapes=None, args=None)\n",
      "    Creates a `Dataset` whose elements are generated by `generator`.\n",
      "    \n",
      "    The `generator` argument must be a callable object that returns\n",
      "    an object that support the `iter()` protocol (e.g. a generator function).\n",
      "    The elements generated by `generator` must be compatible with the given\n",
      "    `output_types` and (optional) `output_shapes` arguments.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    import itertools\n",
      "    tf.compat.v1.enable_eager_execution()\n",
      "    \n",
      "    def gen():\n",
      "      for i in itertools.count(1):\n",
      "        yield (i, [1] * i)\n",
      "    \n",
      "    ds = tf.data.Dataset.from_generator(\n",
      "        gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\n",
      "    \n",
      "    for value in ds.take(2):\n",
      "      print value\n",
      "    # (1, array([1]))\n",
      "    # (2, array([1, 1]))\n",
      "    ```\n",
      "    \n",
      "    NOTE: The current implementation of `Dataset.from_generator()` uses\n",
      "    `tf.compat.v1.py_func` and inherits the same constraints. In particular, it\n",
      "    requires the `Dataset`- and `Iterator`-related operations to be placed\n",
      "    on a device in the same process as the Python program that called\n",
      "    `Dataset.from_generator()`. The body of `generator` will not be\n",
      "    serialized in a `GraphDef`, and you should not use this method if you\n",
      "    need to serialize your model and restore it in a different environment.\n",
      "    \n",
      "    NOTE: If `generator` depends on mutable global variables or other external\n",
      "    state, be aware that the runtime may invoke `generator` multiple times\n",
      "    (in order to support repeating the `Dataset`) and at any time\n",
      "    between the call to `Dataset.from_generator()` and the production of the\n",
      "    first element from the generator. Mutating global variables or external\n",
      "    state can cause undefined behavior, and we recommend that you explicitly\n",
      "    cache any external state in `generator` before calling\n",
      "    `Dataset.from_generator()`.\n",
      "    \n",
      "    Args:\n",
      "      generator: A callable object that returns an object that supports the\n",
      "        `iter()` protocol. If `args` is not specified, `generator` must take no\n",
      "        arguments; otherwise it must take as many arguments as there are values\n",
      "        in `args`.\n",
      "      output_types: A nested structure of `tf.DType` objects corresponding to\n",
      "        each component of an element yielded by `generator`.\n",
      "      output_shapes: (Optional.) A nested structure of `tf.TensorShape` objects\n",
      "        corresponding to each component of an element yielded by `generator`.\n",
      "      args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
      "        and passed to `generator` as NumPy-array arguments.\n",
      "    \n",
      "    Returns:\n",
      "      Dataset: A `Dataset`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.data.Dataset.from_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements_gen():\n",
    "    text = [[1, 2, 3], [3, 4,16,12,777], [1, 2], [8, 9, 0],[1, 2], [8, 9, 0],\n",
    "           [1,2,3,4],[90,50]]\n",
    "    label = [1, 2, 1, 2]\n",
    "    for x, y in zip(text, label):\n",
    "        yield (x, y)\n",
    "\n",
    "def element_length_fn(x, y):\n",
    "    return tf.shape(x)[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator=elements_gen,\n",
    "                                     output_shapes=([None],[]),\n",
    "                                     output_types=(tf.int32, tf.int32))\n",
    "\n",
    "dataset = dataset.apply(tf.data.experimental.\\\n",
    "                        bucket_by_sequence_length(element_length_func=element_length_fn,\n",
    "                                                              bucket_batch_sizes=[1,2], \n",
    "                                                              bucket_boundaries=[3]))\n",
    "\n",
    "# bucket batch size second element defines how many samples per batch\n",
    "\n",
    "# upper length boundary refers to the minimum allowable length of a seq \n",
    "\n",
    "batch = dataset.make_one_shot_iterator().get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=7145, shape=(2, 5), dtype=int32, numpy=\n",
       " array([[  1,   2,   3,   0,   0],\n",
       "        [  3,   4,  16,  12, 777]], dtype=int32)>,\n",
       " <tf.Tensor: id=7146, shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements_gen():\n",
    "    text = [[1, 2, 3], [3, 4,16,12,777], [1, 2], [8, 9, 0],[1, 2], [8, 9, 0],\n",
    "           [1,2,3,4],[90,50]]\n",
    "    label = [1, 2, 1, 2]\n",
    "    for x, y in zip(text, label):\n",
    "        yield (x, y)\n",
    "\n",
    "def element_length_fn(x, y):\n",
    "    return tf.shape(x)[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(generator=elements_gen,\n",
    "                                     output_shapes=([None],[]),\n",
    "                                     output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((None,), ()), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
