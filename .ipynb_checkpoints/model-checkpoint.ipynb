{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as thub\n",
    "import bert\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import model_utils\n",
    "\n",
    "# maximum length of token sequences to input to bert model\n",
    "max_seq_length = 128\n",
    "\n",
    "# number of smaples to generate from the distributed database\n",
    "n_samples = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  1 19:04:37 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   35C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           On   | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   69C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Verify NVIDIA drivers are install successfully\n",
    "\n",
    "!sudo nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-308e466c0911>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm gpu is available to tf\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(context):\n",
    "    \n",
    "    \"\"\"\n",
    "    To be applied over Spark dataframe.\n",
    "    Takes a string and converts it to token IDs via bert_tokenizer,\n",
    "    adding the necessary beginning and end tokens\n",
    "\n",
    "    Returns: Array of bert token ids for each row of Spark dataframe (requires udf)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized = [\"[CLS]\"] + tokenizer.tokenize(context) + [\"[SEP]\"]\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_df(sarcastic, non_sarcastic, ratio, n_samples, max_seq_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns: Spark df of equal label distribution with text \n",
    "    tokenized. Each generated df is to be iterator over multiple \n",
    "    times during training\n",
    "    \"\"\"\n",
    "    \n",
    "    number = 0\n",
    "    while number < n_samples:\n",
    "        \n",
    "        non_sarc_samp = non_sarcastic.sample(ratio) # making label dist equal\n",
    "        \n",
    "        # combine sampled non_sarcastic and whole sarcastic\n",
    "        sample_df = sarcastic.union(non_sarc_samp)\n",
    "        \n",
    "        # tokenize context column via spark udf\n",
    "        tokenize_sample_udf = F.udf(tokenize_sample, ArrayType(IntegerType()))\n",
    "        sample_df = sample_df.withColumn(\"raw_tokens\", tokenize_sample_udf(sample_df.context))\n",
    "        # keep only the first 'max_seq_length' tokens\n",
    "        sample_df = sample_df.withColumn(\"tokens\", F.slice('raw_tokens',1, max_seq_length))\n",
    "        \n",
    "        # drop context and raw_tokens columns\n",
    "        sample_df = sample_df.drop(\"context\")\n",
    "        sample_df = sample_df.drop(\"raw_tokens\")\n",
    "        \n",
    "        # yield one call at a time\n",
    "        yield sample_df\n",
    "        number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "        n_fine_tune_layers=2,\n",
    "        output_type=\"sequence_output\",\n",
    "        bert_path=\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs):\n",
    "        \n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.output_type = output_type\n",
    "        self.bert_path = bert_path\n",
    "        \n",
    "        if self.output_type not in [\"sequence_output\", \"pooled_output\"]:\n",
    "            raise NameError(\"Undefined pooling type (must be either sequence_output or pooled_output, but is {self.output_type}\")\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def layer_number(self, var):\n",
    "    \n",
    "        \"\"\"\n",
    "        Get the layer number corresponding to the \n",
    "        given variable\n",
    "        \"\"\"\n",
    "        m = re.search(r'/layer_(\\d+)/', var)\n",
    "        \n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        \"\"\"\n",
    "        Creates the variables of the layer (optional, for subclass implementers).\n",
    " |      \n",
    " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
    " |      can override if they need a state-creation step in-between\n",
    " |      layer instantiation and layer call.\n",
    " |      \n",
    " |      This is typically used to create the weights of `Layer` subclasses.\n",
    " \n",
    "        Called once from `__call__`, when we know the shapes of input and `dtype`.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.bert_model = thub.KerasLayer(self.bert_path, self.trainable)\n",
    "\n",
    "        # extract all trainable variables from the model\n",
    "        trainable_vars = self.bert_model.trainable_variables\n",
    "        \n",
    "        if self.output_type == \"pooled_output\":\n",
    "            \n",
    "            # removing '/cls/' layers (there don't appear to be any) \n",
    "            trainable_vars = [var.name for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            \n",
    "        elif self.output_type == \"sequence_output\":\n",
    "            \n",
    "            # removing '/cls/' (there don't appear to be any) and '/pooler_transform/' layers \n",
    "            trainable_vars = [var.name for var in trainable_vars if not \"/cls/\" in var.name\n",
    "                              and not \"/pooler_transform\" in var.name] \n",
    "            \n",
    "        ### select how many layers to fine tune starting from top-most layer ###\n",
    "        \n",
    "        # outputs a list of either Nonetype or layer number\n",
    "        layer_numbers = list(map(self.layer_number, trainable_vars))\n",
    "        # returns the total number of layers in pre-trained model (note: layers are zero-indexed)\n",
    "        n_total_layers = max(n for n in layer_numbers if n is not None) + 1 \n",
    "        # finally, create list of just layers to be trained\n",
    "        trainable_vars = [var for n, var in zip(layer_numbers, trainable_vars) if n is not None and n >= n_total_layers - self.n_fine_tune_layers]    \n",
    "        \n",
    "        # add variables NOT to be trained to _non_trainable_weights and \n",
    "        # remove them from _trainable_weights\n",
    "        # note: underscore is necessary for accessing the writable object\n",
    "        for var in self.bert_model.variables:\n",
    "\n",
    "            if var.name not in trainable_vars and \"Variable:0\" not in var.name:\n",
    "                    \n",
    "                    # add non_trainable weights to _non_trainable_weights\n",
    "                    self.bert_model._non_trainable_weights.append(var)\n",
    "                    \n",
    "                    ### due to boolean-related issues with nparrays, need to take different approach to removing weights ###\n",
    "                    # pull out index in _trainable_weights corresponding to the var.name that needs to be removed\n",
    "                    ix = [(i,j.name) for i,j in enumerate(self.bert_model._trainable_weights) if j.name == var.name][0][0]\n",
    "                    # pop it off\n",
    "                    self.bert_model._trainable_weights.pop(ix)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs): \n",
    "        \n",
    "        \"\"\"\n",
    "        Called in `__call__` after making sure `build()` has been called\n",
    " |      once. Should actually perform the logic of applying the layer to the\n",
    " |      input tensors (which should be passed in as the first argument).\n",
    "        \"\"\"\n",
    "        # takes in list of input tensors and casts them in Keras\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        \n",
    "        #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
    "        if self.output_type == \"pooled_output\":\n",
    "            \n",
    "            output = self.bert_model(inputs)[0]\n",
    "                \n",
    "        elif self.output_type == \"sequence_output\":\n",
    "            \n",
    "            output = self.bert_model(inputs)[1]\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        return (input_shape[0], self.output_size)\n",
    "\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                max_seq_length=128,\n",
    "                n_epochs=5,\n",
    "                batch_size=13,\n",
    "                patience=5,\n",
    "                validation_split=0.1,\n",
    "                checkpoint_dir=os.getcwd() + \"/checkpoints\",\n",
    "                saved_model_dir=os.getcwd() + \"/saved_models\",\n",
    "                pad_by_batch=False):\n",
    "    \n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.validation_split = validation_split\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "        self.pad_by_batch = pad_by_batch\n",
    "\n",
    "    def build_model(self, gpu=True): \n",
    "    \n",
    "        \"\"\"\n",
    "        Defines input shapes of bert input tensors,\n",
    "        \"\"\"\n",
    "        input_token_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "        bert_inputs = [input_token_ids, input_mask, segment_ids]\n",
    "    \n",
    "        if gpu == True:\n",
    "            \n",
    "            \"\"\"\n",
    "            This will create a MirroredStrategy instance which will use \n",
    "            all the GPUs that are visible to TensorFlow, and use NCCL as \n",
    "            the cross device communication.\n",
    "            \"\"\"\n",
    "            \n",
    "            mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "            with mirrored_strategy.scope():\n",
    "                \n",
    "                # tf graph\n",
    "                bert_output = BertLayer()(bert_inputs)\n",
    "                dense_out = tf.keras.layers.Dense(self.max_seq_length, activation='relu')(bert_output)\n",
    "                dense_out = tf.keras.layers.Dropout(0.5)(dense_out)\n",
    "                logits = tf.keras.layers.Dense(1, activation='sigmoid')(dense_out)\n",
    "            \n",
    "                # define, compile model\n",
    "                model = tf.keras.models.Model(inputs=bert_inputs, outputs=logits)\n",
    "                model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "                model.summary()\n",
    "        \n",
    "        elif gpu == False:\n",
    "            \n",
    "            # tf graph\n",
    "            bert_output = BertLayer()(bert_inputs)\n",
    "            dense_out = tf.keras.layers.Dense(self.max_seq_length, activation='relu')(bert_output)\n",
    "            dense_out = tf.keras.layers.Dropout(0.5)(dense_out)\n",
    "            logits = tf.keras.layers.Dense(1, activation='sigmoid')(dense_out)\n",
    "    \n",
    "            # define, compile model\n",
    "            model = tf.keras.models.Model(inputs=bert_inputs, outputs=logits)\n",
    "            model.compile(loss='binary_crossentropy', \n",
    "                    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "            model.summary()\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, model, train_inputs, train_labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initiates training process using self.build_model output as input.\n",
    "        If self.pad_by_batch == False, input should be the entire epoch of \n",
    "        training inputs (including masks) and labels, as numpy arrays.\n",
    "        \"\"\"\n",
    "    \n",
    "        checkpoints = tf.keras.callbacks.ModelCheckpoint(self.checkpoint_dir, verbose=1, \n",
    "                                                     save_best_only=False,\n",
    "                                                     save_weights_only=True, mode='auto', \n",
    "                                                     save_freq='epoch')\n",
    "    \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=self.patience)\n",
    "    \n",
    "        if self.pad_by_batch == False:\n",
    "            \n",
    "            train_input_ids, train_input_masks, train_segment_ids = train_inputs\n",
    "        \n",
    "            model.fit([train_input_ids, train_input_masks, train_segment_ids], \n",
    "                  train_labels,\n",
    "                  validation_split = self.validation_split,\n",
    "                  epochs = self.n_epochs,\n",
    "                  batch_size = self.batch_size,\n",
    "                  callbacks = [checkpoints, early_stopping])\n",
    "            \n",
    "            print(\"fitting done; saving model\")\n",
    "    \n",
    "            model.save(self.saved_model_dir+'/my_model.h5')\n",
    "        \n",
    "        \"\"\"\n",
    "        ### HOLDING FOR: Training model on dataset padded is done by batch length ###\n",
    "        \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nneeded to adjust for loading saved model\\n\\nimported = tf.saved_model.load(saved_model_dir+\\'/my_model.h5\\')\\n\\n# Let\\'s take a look to see how many layers are in the base model\\nprint(\"Number of layers in the base model: \", len(base_model.layers))\\n\\n# Fine-tune from this layer onwards\\nfine_tune_at = 100\\n\\n# Freeze all the layers before the `fine_tune_at` layer\\nfor layer in base_model.layers[:fine_tune_at]:\\n  layer.trainable =  False\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "needed to adjust for loading saved model\n",
    "\n",
    "imported = tf.saved_model.load(saved_model_dir+'/my_model.h5')\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 2.12 s, total: 13.2 s\n",
      "Wall time: 15.1 s\n",
      "CPU times: user 51.4 ms, sys: 34.5 ms, total: 85.9 ms\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT model and tokenizer\n",
    "\n",
    "%time bert_layer, tokenizer = model_utils.init_bert()\n",
    "\n",
    "# Initialize Spark context\n",
    "\n",
    "%time sc, spark = model_utils.init_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.7 ms, sys: 30.2 ms, total: 67.9 ms\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "# Read in sarcastic samples, non-sarcastic samples, and the ratio between the two\n",
    "\n",
    "%time sarcastic, non_sarcastic, ratio = model_utils.load_data(spark, \\\n",
    "                                                              bucket_name=\"sarc-bucket-5\", \\\n",
    "                                                              dataset=\"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 3 µs, total: 15 µs\n",
      "Wall time: 19.1 µs\n"
     ]
    }
   ],
   "source": [
    "# Initialize sample_df generator\n",
    "\n",
    "%time sample_generator = generate_sample_df(sarcastic, non_sarcastic, ratio, n_samples, max_seq_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 281 ms, sys: 2.15 ms, total: 283 ms\n",
      "Wall time: 334 ms\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|27265|\n",
      "|    0|27359|\n",
      "+-----+-----+\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_1 (BertLayer)        (None, None, 768)    108310273   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 128)    98432       bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 128)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 1)      129         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,408,834\n",
      "Trainable params: 14,274,305\n",
      "Non-trainable params: 94,134,529\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 9.44 s, sys: 802 ms, total: 10.2 s\n",
      "Wall time: 11 s\n",
      "CPU times: user 15.9 s, sys: 2.14 s, total: 18 s\n",
      "Wall time: 56.8 s\n",
      "CPU times: user 55.4 ms, sys: 28.5 ms, total: 83.9 ms\n",
      "Wall time: 41.7 ms\n",
      "CPU times: user 46.9 ms, sys: 24.9 ms, total: 71.8 ms\n",
      "Wall time: 35.7 ms\n",
      "Train on 49182 samples, validate on 5465 samples\n",
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49179/49182 [============================>.] - ETA: 0s - loss: 0.5883 - accuracy: 0.6795WARNING:tensorflow:5 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x7f136491e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x7f136491e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f136491e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f136491e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: saving model to /sarcasm-detection/checkpoints\n",
      "49182/49182 [==============================] - 887s 18ms/sample - loss: 0.5883 - accuracy: 0.6795 - val_loss: 0.5658 - val_accuracy: 0.7096\n",
      "Epoch 2/50\n",
      "49179/49182 [============================>.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7503\n",
      "Epoch 00002: saving model to /sarcasm-detection/checkpoints\n",
      "49182/49182 [==============================] - 862s 18ms/sample - loss: 0.5130 - accuracy: 0.7502 - val_loss: 0.5488 - val_accuracy: 0.7267\n",
      "Epoch 3/50\n",
      "49179/49182 [============================>.] - ETA: 0s - loss: 0.4699 - accuracy: 0.7788\n",
      "Epoch 00003: saving model to /sarcasm-detection/checkpoints\n",
      "49182/49182 [==============================] - 864s 18ms/sample - loss: 0.4699 - accuracy: 0.7788 - val_loss: 0.5505 - val_accuracy: 0.7327\n",
      "Epoch 4/50\n",
      "  481/49182 [..............................] - ETA: 13:36 - loss: 0.4076 - accuracy: 0.8087\n",
      "Epoch 00004: saving model to /sarcasm-detection/checkpoints\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  481/49182 [..............................] - ETA: 22:07 - loss: 0.4076 - accuracy: 0.8087"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4745a0856b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Execute training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-aa8181635b2b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, train_inputs, train_labels)\u001b[0m\n\u001b[1;32m    100\u001b[0m                   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                   callbacks = [checkpoints, early_stopping])\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_samples):\n",
    "\n",
    "    # Output first smaple\n",
    "\n",
    "    %time sample_df = next(sample_generator)\n",
    "\n",
    "    # Sample df distribution \n",
    "\n",
    "    sample_df.groupBy('label').count().show()\n",
    "\n",
    "    # Initialize training class object and build Bert layer\n",
    "\n",
    "    t = training()\n",
    "    %time model = t.build_model()\n",
    "\n",
    "    # Produce padded tokens, input masks, and segment ids as nparrays\n",
    "\n",
    "    %time padded_tokens, train_labels = model_utils.pad(sample_df, t.batch_size, pad_by_batch=False)\n",
    "    %time input_mask = model_utils.input_mask(padded_tokens)\n",
    "    %time segment_id = model_utils.segment_id(padded_tokens)\n",
    "    train_inputs = [padded_tokens, input_mask, segment_id]\n",
    "\n",
    "    # Execute training\n",
    "\n",
    "    t.train_model(model, train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark context\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
