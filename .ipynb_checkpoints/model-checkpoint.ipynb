{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as thub\n",
    "import bert\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import model_utils\n",
    "\n",
    "# maximum length of token sequences to input to bert model\n",
    "max_seq_length = 128\n",
    "# number of smaples to generate from the distributed database\n",
    "n_samples = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(context):\n",
    "    \n",
    "    \"\"\"\n",
    "    To be applied over Spark dataframe.\n",
    "    Takes a string and converts it to token IDs via bert_tokenizer,\n",
    "    adding the necessary beginning and end tokens\n",
    "\n",
    "    Returns: Array of bert token ids for each row of Spark dataframe (requires udf)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized = [\"[CLS]\"] + tokenizer.tokenize(context) + [\"[SEP]\"]\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_df(sarcastic, non_sarcastic, ratio, n_samples):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns: Spark df of equal label distribution with text \n",
    "    tokenized. Each generated df is to be iterator over multiple \n",
    "    times during training\n",
    "    \"\"\"\n",
    "    \n",
    "    number = 0\n",
    "    while number < n_samples:\n",
    "        non_sarc_samp = non_sarcastic.sample(ratio) # making label dist equal\n",
    "        \n",
    "        # combine sampled non_sarcastic and whole sarcastic\n",
    "        sample_df = sarcastic.union(non_sarc_samp)\n",
    "        \n",
    "        # tokenize context column via spark udf\n",
    "        tokenize_sample_udf = F.udf(tokenize_sample, ArrayType(IntegerType()))\n",
    "        sample_df = sample_df.withColumn(\"tokens\", tokenize_sample_udf(sample_df.context))\n",
    "        \n",
    "        # drop context column\n",
    "        sample_df = sample_df.drop(\"context\")\n",
    "        \n",
    "        # yield one call at a time\n",
    "        yield sample_df\n",
    "        number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "        n_fine_tune_layers=2,\n",
    "        output_type=\"sequence_output\",\n",
    "        bert_path=\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs):\n",
    "        \n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.output_type = output_type\n",
    "        self.bert_path = bert_path\n",
    "        \n",
    "        if self.output_type not in [\"sequence_output\", \"pooled_output\"]:\n",
    "            raise NameError(\"Undefined pooling type (must be either sequence_output or pooled_output, but is {self.output_type}\")\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def layer_number(self, var):\n",
    "    \n",
    "        \"\"\"\n",
    "        Get the layer number corresponding to the \n",
    "        given variable\n",
    "        \"\"\"\n",
    "        m = re.search(r'/layer_(\\d+)/', var)\n",
    "        \n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        \"\"\"\n",
    "        Creates the variables of the layer (optional, for subclass implementers).\n",
    " |      \n",
    " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
    " |      can override if they need a state-creation step in-between\n",
    " |      layer instantiation and layer call.\n",
    " |      \n",
    " |      This is typically used to create the weights of `Layer` subclasses.\n",
    " \n",
    "        Called once from `__call__`, when we know the shapes of input and `dtype`.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.bert_model = thub.KerasLayer(self.bert_path, self.trainable)\n",
    "\n",
    "        # extract all trainable variables from the model\n",
    "        trainable_vars = self.bert_model.trainable_variables\n",
    "        \n",
    "        if self.output_type == \"pooled_output\":\n",
    "            \n",
    "            # removing '/cls/' layers (there don't appear to be any) \n",
    "            trainable_vars = [var.name for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            \n",
    "        elif self.output_type == \"sequence_output\":\n",
    "            \n",
    "            # removing '/cls/' (there don't appear to be any) and '/pooler_transform/' layers \n",
    "            trainable_vars = [var.name for var in trainable_vars if not \"/cls/\" in var.name\n",
    "                              and not \"/pooler_transform\" in var.name] \n",
    "            \n",
    "        ### select how many layers to fine tune starting from top-most layer ###\n",
    "        \n",
    "        # outputs a list of either Nonetype or layer number\n",
    "        layer_numbers = list(map(self.layer_number, trainable_vars))\n",
    "        # returns the total number of layers in pre-trained model (note: layers are zero-indexed)\n",
    "        n_total_layers = max(n for n in layer_numbers if n is not None) + 1 \n",
    "        # finally, create list of just layers to be trained\n",
    "        trainable_vars = [var for n, var in zip(layer_numbers, trainable_vars) if n is not None and n >= n_total_layers - self.n_fine_tune_layers]    \n",
    "        \n",
    "        # add variables NOT to be trained to _non_trainable_weights and \n",
    "        # remove them from _trainable_weights\n",
    "        # note: underscore is necessary for accessing the writable object\n",
    "        for var in self.bert_model.variables:\n",
    "\n",
    "            if var.name not in trainable_vars and \"Variable:0\" not in var.name:\n",
    "                \n",
    "                    self.bert_model._non_trainable_weights.append(var)\n",
    "                    self.bert_model._trainable_weights.remove(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs): \n",
    "        \n",
    "        \"\"\"\n",
    "        Called in `__call__` after making sure `build()` has been called\n",
    " |      once. Should actually perform the logic of applying the layer to the\n",
    " |      input tensors (which should be passed in as the first argument).\n",
    "        \"\"\"\n",
    "        # takes in list of input tensors and casts them in Keras\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        \n",
    "        #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
    "        if self.output_type == \"pooled_output\":\n",
    "            \n",
    "            output = self.bert_model(inputs)[0]\n",
    "                \n",
    "        elif self.output_type == \"sequence_output\":\n",
    "            \n",
    "            output = self.bert_model(inputs)[1]\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \n",
    "        return (input_shape[0], self.output_size)\n",
    "\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                max_seq_length=128,\n",
    "                n_epochs=50,\n",
    "                batch_size=13,\n",
    "                patience=5,\n",
    "                validation_split=0.1,\n",
    "                checkpoint_dir=os.getcwd() + \"/checkpoints\",\n",
    "                saved_model_dir=os.getcwd() + \"/saved_models\",\n",
    "                pad_by_batch=False):\n",
    "    \n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.validation_split = validation_split\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "        self.pad_by_batch = pad_by_batch\n",
    "\n",
    "    def build_model(self): \n",
    "    \n",
    "        \"\"\"\n",
    "        Defines input shapes of bert input tensors,\n",
    "        \"\"\"\n",
    "        input_word_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "        input_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "        segment_ids = tf.keras.layers.Input(shape=(self.max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "        bert_inputs = [input_word_ids, input_mask, segment_ids]\n",
    "    \n",
    "        bert_output = BertLayer()(bert_inputs)\n",
    "        dense_out = tf.keras.layers.Dense(self.max_seq_length, activation='relu')(bert_output)\n",
    "        dense_out = tf.keras.layers.Dropout(0.5)(dense_out)\n",
    "        logits = tf.keras.layers.Dense(1, activation='sigmoid')(dense_out)\n",
    "    \n",
    "        model = tf.keras.models.Model(inputs=bert_inputs, outputs=logits)\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "        model.summary()\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, model, train_inputs, train_labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initiates training process using self.build_model output as input.\n",
    "        If self.pad_by_batch == False, input should be the entire epoch of \n",
    "        training inputs (including masks) and labels, as numpy arrays.\n",
    "        \"\"\"\n",
    "    \n",
    "        checkpoints = tf.keras.callbacks.ModelCheckpoint(self.checkpoint_dir, verbose=1, \n",
    "                                                     save_best_only=False,\n",
    "                                                     save_weights_only=True, mode='auto', \n",
    "                                                     save_freq='epoch')\n",
    "    \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=self.patience)\n",
    "    \n",
    "        if self.pad_by_batch == False:\n",
    "            \n",
    "            train_input_ids, train_input_masks, train_segment_ids = train_inputs\n",
    "        \n",
    "            model.fit([train_input_ids, train_input_masks, train_segment_ids], \n",
    "                  train_labels,\n",
    "                  validation_split = self.validation_split,\n",
    "                  epochs = self.n_epochs,\n",
    "                  batch_size = self.batch_size,\n",
    "                  callbacks = [checkpoints, early_stopping])\n",
    "    \n",
    "            model.save(self.saved_model_dir+'/my_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.33 s, sys: 642 ms, total: 6.97 s\n",
      "Wall time: 6.89 s\n",
      "CPU times: user 33.6 ms, sys: 13.4 ms, total: 47 ms\n",
      "Wall time: 9.98 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT model and tokenizer\n",
    "\n",
    "%time bert_layer, tokenizer = model_utils.init_bert()\n",
    "\n",
    "# Initialize Spark context\n",
    "\n",
    "%time sc, spark = model_utils.init_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 3.37 ms, total: 14.3 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "# Read in sarcastic samples, non-sarcastic samples, and the ratio between the two\n",
    "\n",
    "%time sarcastic, non_sarcastic, ratio = model_utils.load_data(spark, \\\n",
    "                                                              bucket_name=\"sarc-bucket-5\", \\\n",
    "                                                              dataset=\"politics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 16.7 µs\n",
      "CPU times: user 267 ms, sys: 7.2 ms, total: 274 ms\n",
      "Wall time: 369 ms\n"
     ]
    }
   ],
   "source": [
    "# Initialize sample_df generator\n",
    "\n",
    "%time sample_generator = generate_sample_df(sarcastic, non_sarcastic, ratio, n_samples)\n",
    "\n",
    "# Output first smaple\n",
    "\n",
    "%time sample_df = next(sample_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>>: ValueError: Failed to parse source code of <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>>, which Python reported as:\n",
      "   def call(self, inputs): \n",
      "\n",
      "       \"\"\"\n",
      "       Called in `__call__` after making sure `build()` has been called\n",
      "|      once. Should actually perform the logic of applying the layer to the\n",
      "|      input tensors (which should be passed in as the first argument).\n",
      "       \"\"\"\n",
      "       # takes in list of input tensors and casts them in Keras\n",
      "       inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
      "\n",
      "       #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
      "       if self.output_type == \"pooled_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[0]\n",
      "\n",
      "       elif self.output_type == \"sequence_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[1]\n",
      "\n",
      "       return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>>: ValueError: Failed to parse source code of <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>>, which Python reported as:\n",
      "   def call(self, inputs): \n",
      "\n",
      "       \"\"\"\n",
      "       Called in `__call__` after making sure `build()` has been called\n",
      "|      once. Should actually perform the logic of applying the layer to the\n",
      "|      input tensors (which should be passed in as the first argument).\n",
      "       \"\"\"\n",
      "       # takes in list of input tensors and casts them in Keras\n",
      "       inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
      "\n",
      "       #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
      "       if self.output_type == \"pooled_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[0]\n",
      "\n",
      "       elif self.output_type == \"sequence_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[1]\n",
      "\n",
      "       return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>>: ValueError: Failed to parse source code of <bound method BertLayer.call of <__main__.BertLayer object at 0x66080b650>>, which Python reported as:\n",
      "   def call(self, inputs): \n",
      "\n",
      "       \"\"\"\n",
      "       Called in `__call__` after making sure `build()` has been called\n",
      "|      once. Should actually perform the logic of applying the layer to the\n",
      "|      input tensors (which should be passed in as the first argument).\n",
      "       \"\"\"\n",
      "       # takes in list of input tensors and casts them in Keras\n",
      "       inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
      "\n",
      "       #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
      "       if self.output_type == \"pooled_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[0]\n",
      "\n",
      "       elif self.output_type == \"sequence_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[1]\n",
      "\n",
      "       return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, None, 768)    108310273   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 128)    98432       bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 128)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1)      129         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 108,408,834\n",
      "Trainable params: 14,274,305\n",
      "Non-trainable params: 94,134,529\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 8.52 s, sys: 994 ms, total: 9.51 s\n",
      "Wall time: 9.75 s\n"
     ]
    }
   ],
   "source": [
    "# Initialize training class object and build Bert layer\n",
    "\n",
    "t = training()\n",
    "%time model = t.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce padded tokens, input masks, and segment ids as nparrays\n",
    "\n",
    "%time padded_tokens, train_labels = model_utils.pad(sample_df, pad_by_batch=False, t.batch_size)\n",
    "\n",
    "%time input_mask = model_utils.input_mask(padded_tokens)\n",
    "\n",
    "%time segment_id = model_utils.segment_id(padded_tokens)\n",
    "\n",
    "train_inputs = [padded_tokens, input_mask, segment_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>>: ValueError: Failed to parse source code of <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>>, which Python reported as:\n",
      "   def call(self, inputs): \n",
      "\n",
      "       \"\"\"\n",
      "       Called in `__call__` after making sure `build()` has been called\n",
      "|      once. Should actually perform the logic of applying the layer to the\n",
      "|      input tensors (which should be passed in as the first argument).\n",
      "       \"\"\"\n",
      "       # takes in list of input tensors and casts them in Keras\n",
      "       inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
      "\n",
      "       #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
      "       if self.output_type == \"pooled_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[0]\n",
      "\n",
      "       elif self.output_type == \"sequence_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[1]\n",
      "\n",
      "       return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>>: ValueError: Failed to parse source code of <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>>, which Python reported as:\n",
      "   def call(self, inputs): \n",
      "\n",
      "       \"\"\"\n",
      "       Called in `__call__` after making sure `build()` has been called\n",
      "|      once. Should actually perform the logic of applying the layer to the\n",
      "|      input tensors (which should be passed in as the first argument).\n",
      "       \"\"\"\n",
      "       # takes in list of input tensors and casts them in Keras\n",
      "       inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
      "\n",
      "       #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
      "       if self.output_type == \"pooled_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[0]\n",
      "\n",
      "       elif self.output_type == \"sequence_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[1]\n",
      "\n",
      "       return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>>: ValueError: Failed to parse source code of <bound method BertLayer.call of <__main__.BertLayer object at 0x99805b7d0>>, which Python reported as:\n",
      "   def call(self, inputs): \n",
      "\n",
      "       \"\"\"\n",
      "       Called in `__call__` after making sure `build()` has been called\n",
      "|      once. Should actually perform the logic of applying the layer to the\n",
      "|      input tensors (which should be passed in as the first argument).\n",
      "       \"\"\"\n",
      "       # takes in list of input tensors and casts them in Keras\n",
      "       inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
      "\n",
      "       #bert-for-tf2 returns (pooled_output,sequence_output) when called\n",
      "       if self.output_type == \"pooled_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[0]\n",
      "\n",
      "       elif self.output_type == \"sequence_output\":\n",
      "\n",
      "           output = self.bert_model(inputs)[1]\n",
      "\n",
      "       return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_18 (BertLayer)       (None, None, 768)    108310273   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 128)    98432       bert_layer_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 128)    0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 1)      129         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 108,408,834\n",
      "Trainable params: 14,274,305\n",
      "Non-trainable params: 94,134,529\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t.train_model(model, train_inputs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'bert_layer_8/bert_model/pooler_transform/kernel:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_vars = ['bert_layer_8/bert_model/encoder/layer_10/self_attention/query/kernel:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention/query/bias:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention/key/kernel:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention/key/bias:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention/value/kernel:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention/value/bias:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention_output/kernel:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention_output/bias:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention_layer_norm/gamma:0', 'bert_layer_8/bert_model/encoder/layer_10/self_attention_layer_norm/beta:0', 'bert_layer_8/bert_model/encoder/layer_10/intermediate/kernel:0', 'bert_layer_8/bert_model/encoder/layer_10/intermediate/bias:0', 'bert_layer_8/bert_model/encoder/layer_10/output/kernel:0', 'bert_layer_8/bert_model/encoder/layer_10/output/bias:0', 'bert_layer_8/bert_model/encoder/layer_10/output_layer_norm/gamma:0', 'bert_layer_8/bert_model/encoder/layer_10/output_layer_norm/beta:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention/query/kernel:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention/query/bias:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention/key/kernel:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention/key/bias:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention/value/kernel:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention/value/bias:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention_output/kernel:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention_output/bias:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention_layer_norm/gamma:0', 'bert_layer_8/bert_model/encoder/layer_11/self_attention_layer_norm/beta:0', 'bert_layer_8/bert_model/encoder/layer_11/intermediate/kernel:0', 'bert_layer_8/bert_model/encoder/layer_11/intermediate/bias:0', 'bert_layer_8/bert_model/encoder/layer_11/output/kernel:0', 'bert_layer_8/bert_model/encoder/layer_11/output/bias:0', 'bert_layer_8/bert_model/encoder/layer_11/output_layer_norm/gamma:0', 'bert_layer_8/bert_model/encoder/layer_11/output_layer_norm/beta:0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(var_name not in trainable_vars) and ('Variable:0' not in var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer, tokenizer = model_utils.init_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_model/word_embeddings/embeddings:0',\n",
       " 'bert_model/embedding_postprocessor/type_embeddings:0',\n",
       " 'bert_model/embedding_postprocessor/position_embeddings:0',\n",
       " 'bert_model/embedding_postprocessor/layer_norm/gamma:0',\n",
       " 'bert_model/embedding_postprocessor/layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_0/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_0/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_0/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_0/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_0/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_0/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_0/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_0/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_0/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_0/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_0/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_0/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_0/output/kernel:0',\n",
       " 'bert_model/encoder/layer_0/output/bias:0',\n",
       " 'bert_model/encoder/layer_0/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_0/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_1/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_1/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_1/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_1/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_1/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_1/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_1/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_1/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_1/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_1/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_1/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_1/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_1/output/kernel:0',\n",
       " 'bert_model/encoder/layer_1/output/bias:0',\n",
       " 'bert_model/encoder/layer_1/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_1/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_2/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_2/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_2/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_2/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_2/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_2/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_2/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_2/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_2/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_2/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_2/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_2/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_2/output/kernel:0',\n",
       " 'bert_model/encoder/layer_2/output/bias:0',\n",
       " 'bert_model/encoder/layer_2/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_2/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_3/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_3/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_3/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_3/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_3/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_3/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_3/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_3/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_3/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_3/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_3/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_3/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_3/output/kernel:0',\n",
       " 'bert_model/encoder/layer_3/output/bias:0',\n",
       " 'bert_model/encoder/layer_3/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_3/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_4/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_4/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_4/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_4/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_4/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_4/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_4/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_4/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_4/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_4/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_4/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_4/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_4/output/kernel:0',\n",
       " 'bert_model/encoder/layer_4/output/bias:0',\n",
       " 'bert_model/encoder/layer_4/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_4/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_5/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_5/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_5/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_5/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_5/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_5/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_5/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_5/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_5/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_5/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_5/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_5/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_5/output/kernel:0',\n",
       " 'bert_model/encoder/layer_5/output/bias:0',\n",
       " 'bert_model/encoder/layer_5/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_5/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_6/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_6/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_6/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_6/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_6/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_6/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_6/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_6/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_6/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_6/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_6/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_6/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_6/output/kernel:0',\n",
       " 'bert_model/encoder/layer_6/output/bias:0',\n",
       " 'bert_model/encoder/layer_6/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_6/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_7/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_7/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_7/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_7/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_7/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_7/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_7/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_7/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_7/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_7/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_7/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_7/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_7/output/kernel:0',\n",
       " 'bert_model/encoder/layer_7/output/bias:0',\n",
       " 'bert_model/encoder/layer_7/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_7/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_8/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_8/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_8/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_8/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_8/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_8/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_8/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_8/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_8/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_8/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_8/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_8/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_8/output/kernel:0',\n",
       " 'bert_model/encoder/layer_8/output/bias:0',\n",
       " 'bert_model/encoder/layer_8/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_8/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_9/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_9/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_9/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_9/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_9/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_9/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_9/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_9/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_9/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_9/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_9/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_9/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_9/output/kernel:0',\n",
       " 'bert_model/encoder/layer_9/output/bias:0',\n",
       " 'bert_model/encoder/layer_9/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_9/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_10/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_10/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_10/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_10/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_10/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_10/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_10/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_10/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_10/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_10/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_10/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_10/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_10/output/kernel:0',\n",
       " 'bert_model/encoder/layer_10/output/bias:0',\n",
       " 'bert_model/encoder/layer_10/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_10/output_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_11/self_attention/query/kernel:0',\n",
       " 'bert_model/encoder/layer_11/self_attention/query/bias:0',\n",
       " 'bert_model/encoder/layer_11/self_attention/key/kernel:0',\n",
       " 'bert_model/encoder/layer_11/self_attention/key/bias:0',\n",
       " 'bert_model/encoder/layer_11/self_attention/value/kernel:0',\n",
       " 'bert_model/encoder/layer_11/self_attention/value/bias:0',\n",
       " 'bert_model/encoder/layer_11/self_attention_output/kernel:0',\n",
       " 'bert_model/encoder/layer_11/self_attention_output/bias:0',\n",
       " 'bert_model/encoder/layer_11/self_attention_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_11/self_attention_layer_norm/beta:0',\n",
       " 'bert_model/encoder/layer_11/intermediate/kernel:0',\n",
       " 'bert_model/encoder/layer_11/intermediate/bias:0',\n",
       " 'bert_model/encoder/layer_11/output/kernel:0',\n",
       " 'bert_model/encoder/layer_11/output/bias:0',\n",
       " 'bert_model/encoder/layer_11/output_layer_norm/gamma:0',\n",
       " 'bert_model/encoder/layer_11/output_layer_norm/beta:0',\n",
       " 'bert_model/pooler_transform/kernel:0',\n",
       " 'bert_model/pooler_transform/bias:0',\n",
       " 'Variable:0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var.name for var in bert_layer.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
